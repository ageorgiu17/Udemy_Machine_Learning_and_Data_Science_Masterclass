{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization - Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În partea anterioară am discutat depsre partea de Regulariza, ce înseamnă asta, cum se realizează și care sunt cele trei tipuri de regularizare:\n",
    "\n",
    "- Lasso Regresion\n",
    "\n",
    "- Ridge Regression\n",
    "\n",
    "- Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Înainte de a executa partea aceasta de regularizare trebuie să ne asigurăm că fiecare dintre acestea este rulată pe același set de date. Pentru asta o să lucrăm în Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data into a DataFrame\n",
    "df = pd.read_csv('../data/08-Linear-Regression-Models/Advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "0  230.1   37.8       69.2   22.1\n",
       "1   44.5   39.3       45.1   10.4\n",
       "2   17.2   45.9       69.3    9.3\n",
       "3  151.5   41.3       58.5   18.5\n",
       "4  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the head of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train-test set\n",
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "După ce am împărțit setul de date în labels și features urmează partea în care transformăm features utilizând Polynomial Regression (PolynomialFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the PolynomialFeatures instance\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a polynomial converter\n",
    "polynomial_converter = PolynomialFeatures(degree=3, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating polynomial features using the polynomial converter\n",
    "polynomial_features = polynomial_converter.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.30100000e+02, 3.78000000e+01, 6.92000000e+01, ...,\n",
       "        9.88757280e+04, 1.81010592e+05, 3.31373888e+05],\n",
       "       [4.45000000e+01, 3.93000000e+01, 4.51000000e+01, ...,\n",
       "        6.96564990e+04, 7.99365930e+04, 9.17338510e+04],\n",
       "       [1.72000000e+01, 4.59000000e+01, 6.93000000e+01, ...,\n",
       "        1.46001933e+05, 2.20434291e+05, 3.32812557e+05],\n",
       "       ...,\n",
       "       [1.77000000e+02, 9.30000000e+00, 6.40000000e+00, ...,\n",
       "        5.53536000e+02, 3.80928000e+02, 2.62144000e+02],\n",
       "       [2.83600000e+02, 4.20000000e+01, 6.62000000e+01, ...,\n",
       "        1.16776800e+05, 1.84062480e+05, 2.90117528e+05],\n",
       "       [2.32100000e+02, 8.60000000e+00, 8.70000000e+00, ...,\n",
       "        6.43452000e+02, 6.50934000e+02, 6.58503000e+02]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the polynomial features array\n",
    "polynomial_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasul care urmează este acela de a împărți setul de date în train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the train-test-split method\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(polynomial_features, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Până în acest moment am realizat operațiuni pe care le-am mai folosit anterior. Din acest moment începe procestul prin care se scalează datele respective. Pentru partea de scalare se va folosit tot o instanță din cadrul modulului de preprocesare (skleanr.preprocessing) denumit StandardScaler(). Să importăm acest modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the StandarScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "După ce s-a creat acest scaler, procestul este asemănător cu cel în care se creează features Polynomial. Se creează o instanță pentru un scaler, se oferă un set de date pentru partea de vizualizare (metoda .fit()) după care se transformă un set de date. FOARTE IMPORTANT de reținut însă pentru partea de scalare este faptul că aici nu trebuie să oferim tot setul de date (polynomial_features) pentru partea de vizualizare, deoarece în cadrul acelui set de date se găsesc și datele care o să se utilizeze pentru partea de testare. Nu dorim acest lucru deoarece prin acest procedeul modelul poate să vadă dinainte date care ar trebui utilizate doar pentru partea de testare. Acest concept poartă denumirea de data leakeage. Aici foarte mulți începători fac greșeala să ofere spre vizualizare tot setul de date, dar trebuie oferit doar setul de antrenare (.fit(X_train)). După, pentru partea de transformare se va folosi și setul de antrenare și cel de testare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of StandardScaler()\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the data to the scaler\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data sets\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce anume face acest scaler și de nu este bine să îi oferim date de vizualizare pentru întreg setul de date? Acest scaler calculează anumite medii și deviații standard pentru datele peste care se uită, din acest motiv nu este bine să îi oferim pentru vizualizare și datele pentru partea de testare, deoarece acesta poate sî creeze aceste medii sau deviații standard pentru datele respective și să le țină undeva în memorie până ce se fac predicții și să se folosească de acele valori pentru a face predicții, ceea ce nu este recomandat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Să aruncăm acuma o privire peste setul de date care era înanite și peste setul de date care a fost transformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.30100000e+02, 3.78000000e+01, 6.92000000e+01, 5.29460100e+04,\n",
       "       8.69778000e+03, 1.59229200e+04, 1.42884000e+03, 2.61576000e+03,\n",
       "       4.78864000e+03, 1.21828769e+07, 2.00135918e+06, 3.66386389e+06,\n",
       "       3.28776084e+05, 6.01886376e+05, 1.10186606e+06, 5.40101520e+04,\n",
       "       9.88757280e+04, 1.81010592e+05, 3.31373888e+05])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În primul array din polynomial features putem observa faptul că avem următoarele date. 1.21828769e+07 reprezintă valorea 1.21 înmulțit cu 10 la puterea 7, ceea ce reprezintă 12_100_000, ceeea ce este o valoare extrem de mare. Să ne uiităm peste primul array din datele ce au fost transformate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.91339472,  1.76259419,  0.68168558, -0.96397506,  0.01521221,\n",
       "       -0.29304821,  2.31532893,  1.56001049,  0.36991011, -0.87527811,\n",
       "       -0.53295016, -0.54312331,  0.43542739,  0.18669917, -0.2012045 ,\n",
       "        2.77252075,  2.09516506,  0.89465642,  0.09868885])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În acest output se poate vedea că cel mai mare număr este 2.72, ceea ce nu se compară cu 12.000.000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acum că avem datele scalate putem să trecem la partea de regularizare, ceea ce o să facem în continuare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recapitulare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În cadrul acestui tutorial am învățat următoarele lucruri:\n",
    "\n",
    "    1. De unde să importăm un scaler\n",
    "\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    2. Cum să creem un scaler\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    3. Ce date să oferim pentru vizualizare la o instanță a unui scaler\n",
    "\n",
    "        scaler.fit(X_train)\n",
    "\n",
    "            Pentru partea de vizualizare de date, se oferă doar setul de date de antrenare\n",
    "\n",
    "    4. Cum să transformăm datele utilizând un scaler\n",
    "\n",
    "        X_train = scaler.transform(X_train)\n",
    "\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "            Pentru partea de transformare de date se oferă atât setul de date de antrenare cât și cel de testare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
