{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Classification Metrics - Precision, Recall and F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am învățat până în acest moment cu putem să calculăm acuratețea unui model de clasificare. La această acuratețe apare și problema cu class imbalance, atunci când avem mult prea multe valori doar într-o singură clasă. Pentru a rezolva problema respectivă mai putem să ne folosim de metricii de Recall, Precision și F1 Score (F1 Score care combină partea de Recall și Precision). O să începem cu partea de Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dacă acuratețea lua în calcul și cazurile în care o persoană a fost detectată ca și negativ și era o detecție corectă (True Negative), Recall ia în calcul doar aparițile de Positive ale clasei. Recall pune întrebarea, cât de des sunt detectate cazurile pozitive? Acest metric se calculează după formula:\n",
    "\n",
    "Recall = TP / Total Actual Positives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Actual | Data |\n",
    "| --- | --- | --- |\n",
    "| Predicted | Infected   | Healthy |\n",
    "|  Infected  | 4  | 2  |\n",
    "|  Healthy  |  1 | 93  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În cadrul tabelului de mai sus avem valoarea 4 pentru True Positive (atunci când o persoană s-a detectat ca fiind pozitivă și chiar era pozitivă în realitate). Total Actual Positives se calculează ca fiind totalu de True Positives și False Negatives, iar în situația de mai sus cele două adunate dau valoarea de 5. Prin urmare o să avem ecuația de 4 / 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valoare de Recall pentru cazul de mai sus este de 80%. Recall mai răspunde și la întrebare de câte cazuri sunt relevante? Putem spune că în proporție de 80% găsim cazuri relevante, relevant fiind cazul în care o persoană era cu adevărat infectată (testată pozitiv) și am și prezis acest lucru. Să luăm acuma exemplu în care aveam un model care returna Healthy indiferent de caz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Actual | Data |\n",
    "| --- | --- | --- |\n",
    "| Predicted | Infected   | Healthy |\n",
    "|  Infected  | 0  | 0  |\n",
    "|  Healthy  |  5 | 95  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În cazul de mai sus pentru valoarea de True Positives avem 0, iar pentru totaul de Positive cases avem 5, prin urmare o să avem un rezultat pentru Recall de 0. Asta înseamnă că modelul nostru află 0 cazuri relevante, chiar dacă avem o acuratețe de 95% a modelului. Acest caz ne spune că avem un model care este clar overfit deoarece avem o acuratețe de 95%, dar avem un recall de 0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În continuare o să trecem la partea de Precision. Acest metric răspunde la întrebarea: Când o predicție este pozitivă, de câte ori este aceasta corectă? Precizia (Precision) se calculează după ecuația TP / Total Positive Predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Actual | Data |\n",
    "| --- | --- | --- |\n",
    "| Predicted | Infected   | Healthy |\n",
    "|  Infected  | 4  | 2  |\n",
    "|  Healthy  |  1 | 93  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Să luăm din nou cazul de mai sus. Valoarea pentru True Positives este de 4, iar valoarea pentru Total Positive Predicted este de 6 (4 True Positives + 2 False Positives), prin urmare o să avem ecuația 4 / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 / 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putem să vedem că avem o precizie de 0.66 în acest caz. Ce valoare o să avem atunci când avem un model care returnează tot timpul Healthy (adică testat negativ)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Actual | Data |\n",
    "| --- | --- | --- |\n",
    "| Predicted | Infected   | Healthy |\n",
    "|  Infected  | 0  | 0  |\n",
    "|  Healthy  |  5 | 95  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În cazul de mai sus, atât TP cât și Total Positive Predicted sunt 0, prin urmare precision o să fie nedefinit. Acesta este iar un hint cum că modelul pe care l-am folosit nu este unul bun. Din ce se poate observa, metricii de Recall și Precision nu au acceași problemă precum paradoxul de acuratețe. Recall și Precision ne ajută să aflăm metrici cu privire la cazurile relevante. În funcție de modelul pe care alegem să îl folosim poate exista un trade-off între Precision și Recall care poartă denumirea de ROC Curve (o să învățăm și despre asta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "După cum spuneam mai există un metric de care ne putem folosi, și anume F1 Score. Acest F1 Score combină partea de Recall și Precision și reprezintă media harmonică dintre acestea. Media harmonică este creată în așa fel încât dacă una dintre cele două valori este 0 (Precision sau Recall) și rezultatul acestei medii să fie tot 0. Ecuația pentru a calcula F1 Score este următoarea:\n",
    "\n",
    "F1 Score = (2 x Precision x Recall) / (Precision + Recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
