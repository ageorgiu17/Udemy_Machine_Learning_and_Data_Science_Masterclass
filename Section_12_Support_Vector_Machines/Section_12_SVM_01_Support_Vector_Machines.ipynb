{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suport Vector Machines - SVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suport Vector Machine este unul dintre cel mai complex algoritm de Machine Learning peste care o să ne uităm în cadrul acestui curs, dar totul începe cu o simplă premisă:\n",
    "\n",
    "- Există un hyperplane care să separe clasele eficient?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru a putea înțelege ce anume reprezintă un hyperplane, la ce se referă partea de separare de clase eficient și pentru a răspunde la întrebarea aceasta o să fim nevoiți să ne uităm la istoria acestui algoritm. Acest algoritm este printre cele mai recente peste care o să ne uităm în acest curs. Să aruncăm o privire peste istoria acestui algoritm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În anii 1960 a început evoluția pentru SVMs. Acesta a apărut ca și un model de analiză a imaginilor de către un calculator. Aici a început partea de gândire și lucru la cum s-ar putea utiliza algoritmul de Suport Vector Machines pentru a analiza imagini (recunoașterea unui număr). În aceeași perioadă a apărut o publicare în care se introduce ideea de interpolare geometrică a kernel-ului ca produse interioare într-un spațiu caracteristic. Acest concept este fundamental pentru exitindera cazurilor unde putem utiliza SVMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Munca se continuă în 1974 când se publică articolul 'Theory of Pattern Recognition' care duc mai departe capabilitățile algortmului SMV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În anul 1992 apare o idee de a crea un model de clasificare non-linear prin ducere conceptului de kernel trick la maximum în ceea ce privește hyperplanes. Această idee are un impact uriaș în ceea ce privește Suport vector Machines și o să înțelegem de ce atunci când o să descoperim cum anume putem să ne creem calea către acel kernel trick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În 1996 apare și partea de Regression la Suport Vector Machine care extinde capacitatea acestui algoritm și la task-uri de predicție, de Regression. Atunci când discutăm despre intuiția și teoria din spatele acestor SVMs trebuie să ne reamintim acest progres deoarece o să ne lovim de acest progres în momentul în care o să învățăm mai multe (teorie și practică) despre aceste SVMs. O să trecem de la Maximum Margin Classifiers la Suport Vector Classifiers și în final o să ajungem la Suport Vector Machines. O să începem cu partea teoretică despre Maximum Margin Classifier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pentru a putea înțelege ce anume reprezintă Maximum Margin Classifier trebuie să înțeleg conceptele de Hyperplane și Margins. Într-un spațiu N-dimensional un hyperplane este un subloc afin plat de tip hyperplane de dimeniunea N - 1. De exemplu într-un spațiu 1-D, un hyperplane este un punct, într-un spațiu 2-D, un hyperplane este o linie, iar într-un spațiu 3-D un hyperplane este o suprafață plană. Pentru dimenisunile de 1-D, 2-D și 3-D o să folosim noțiunile de punct, linie și suprafață plată, abia la dimensiuni de 4-D sau mai mari o să utilizăm termenul de hyperplane. Ceea ce stă la baza SVM este utilizarea acestor hyperplanes pentru a crea o separare între clase. O să începem cu un exemplu de date de tipul 1-D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../SS/hyperplane_01.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În imaginea de mai sus avem date care sunt clar separabile. Acest tip de date nu prea o să îl găsim în cadrul unui set de date real. Modelul de mai sus este doar pentru partea de înțelegere."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideea care stă la baza SVM este de a crea un hyperplane care să separe anumite clase. Din moment ce setul de date este de tip 1-D, atunci un Hyperplane pentru acest set de date o să fie un punct. Există un punct pe acea linie pe care putem să îl alegem pentru a separa clasele perfect? Punctul acela o să fie reprezentat de linia albastră punctată. Datele noi adăugate o să fie clasificate în funcție de partea hyperplane-ului pe care cad acele puncte. Întrebarea care apare este unde să se aleagă acel punct pentru a face separarea dintre clase? Datele de mai sus sunt perfect separabile și există mai multe puncte pe care putem să le alegem în urma cărora clasele să fie separate perfect. Trebuie să ne decidem ce hyperplane este cel mai potrivit ca și separator între clase. O să avem nevoie de un tip de metodologie cantitativă pentru a face asta. Putem să alegem separatorul care maximizează marginile dintre clase (de unde și denumirea de Maximum Margin Classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../SS/hyperplane_02.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se poate vedea că acel hyperplane are valoarea maximă către marginile fiecărei clase. Modelul de mai sus reprezintă Maximum Margin Classifier. Această idee se aplică și la dimensiuni mai mari. Mai jos se poate observa cum arată acest algoritm pentru date în 2-D (unde un hyperplane este o linie)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../SS/hyperplane_03.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce se întâmplă atunci când avem un set de date care are și un anumit noise, care nu sunt chiar perfect separabile între ele?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../SS/hyperplane_04.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În imaginea de mai sus avem un set de date care are și un outlier. Punctul acela roșu care este mai apropiat de punctele verzi decât de punctele roșii. În cazul de mai sus, dacă alegem hyperplane-ul să fie între punctul verde și roșu (linia punctată cu albastru), atunci dacă avem un nou punct care este mai apropiat de verde (și știm că este verde în realitate), atunci acest model o să îl clasifice greșit. Dacă alegem ca hyperplane-ul să fie între punctele roșii mai depărtate (cam unde a fost în exemplul anterior), atunci modelul o să clasifice greșit acel outlier. În aceste cazuri trebuie să introducem conceptul de bias-variance tradeoff."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../SS/hyperplane_05.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În imaginea de mai sus avem hyperplane-ul care este situat la mijlocul sitanței dintre clasa roșie și cea verde. Aici în momentul în care se antrenează o să clasifice greșit acel punct roșu (care după hyperplane-ul de mai sus se găsește în clasa verde). Partea bună este faptul că atunci când introducem un nou punct (cel negru) care este mai apropiat de clasa verde, atuci acesta o să fie clasificat corect ca aparținând din clasa verde.\n",
    "\n",
    "Dacă se alegea să se meargă cu hyperplane-ul de mai sus (care era între punctul verde și roșu), atunci acest nou punct (negru) era clasificat greșit ca aparținând din clasa roșie (iar el aparține de clasa verde). Acest lucru se întâmpla deoarece exista pre mult variance pe partea de training (modelul prelua prea mult noise din clasa roșie). Modelul prin care permitem parte de missclasification pentru a adăuga mai mult bias modelului (și pentru a reduce partea de variance - overfitting) poartă denumirea de Suport Vector Classifier. Acest model nu mai utilizează Maximul Margins și utilizează conceptul de Soft Margins prin care permitem misclasificare în partea de training pentru ca la final să avem un model mai bun"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am vizualizat cazuri unde clasele sunt ușor separabile de către acest hyperplane. Ce putem face atunci când chiar dacă permitem misclasificare în partea de training, un hyperplane are o performanță slabă?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../SS/hyperplane_06.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De observat exemplul de mai sus unde dacă creem un hyperplane, oriunde l-am poziționa nu o să putem să creem un model fără prea multe misclasificări. În cazul de față s-ar pute utiliza mai multe hyperplanes pentru a rezolva această problemă, dar la dimensiuni mai mare acest lucru nu este posibil, de aceea nu se folosesc mai multe hyperplanes. Pentru a rezolva problema aceasta o să trecem de la Suport Vector Classifiers la Suport Vector Machines. SVM rezolvă problema unde hyperplane-urile nu fac o treabă bună pentru separarea claselor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM utilizează kernels pentru a proiecta datele într-o dimensiune mai mare pentru a folosi hyperplanes în dimensiuni mai mare ca metodă de rezolvare a acestei probleme."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel-ul ne permite să trecem de la Suport Vector Classifiers la Suport Vector Machines. Există o varietate de kernel-uri pe care le putem utiliza pentru a proiecta datele la o dimensiune mai mare. În exemplul de mai sus după cum am spus nu există niciun hyperplane corect care să ne împartă elementele în cele două clase. Aici intervine Suport Vector Machines prin care utilizăm un kernel și să facem un expand la acele date în altă dimensiune. Putem să alegem un kernel polinomial de grad 2 orin care să transformăm acele date. După ce utilizăm acel kernel, datele o să arate în felul următor:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../SS/hyperplane_07.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "După transformarea pe care am realizat-o acuma putem să utilizăm Support Vector Classifier pentru a separa datele respective. În cadrul acestei introduceri am mai auzit și de termenul de 'kernel trick'. Acesta ține de partea matematică a utilizării kernelului în Support Vector Machines. Partea prin care utilizăm un kernel pentru a transforma un set de date într-o altă dimensiune mai mare nu reprezintă partea de 'kernel trick', ci doar o vizualizare a utilizării acestor kernel-uri. Din punct de vedere matematic, conceptul de 'kernel trick' evită recompunerea datelor în date care se găsesc într-o dimensiune mai mare."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acest kernel trick utilizează 'DOT products' pentru a evita computația grea de a transforma aceste date utilizând un kernel. Se profită de aceste 'dot products ale traspunerii datelor pentru a se realiza acest prodeceu. Pentru mai multe detalii despre partea matematică a acestui 'kernel trick' trebuie revăzută partea în care se discută această parte din curs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
