{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În partea precedentă am aflat informații teoretice despre cum funcționează procesul de Feature Extraction from Text, iar acuma o să realizăm acest procedeu utilizând limbaj de Python, o să creem manual acest procedeu de extracție de informații din text. În partea a doua o să folosim aceste concepte cu Scikit-Learn care poate face automat această treabă. O să lucrăm cu două fișiere de tip text pentru a putea realiza asta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/18-Naive-Bayes-and-NLP/One.txt') as file:\n",
    "    a = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a story about dogs\\nour canine pets\\nDogs are furry animals\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "După cum se poate oberva am citit datele din fișier, însă acestea au fost citite sub formă de un singur string, din acest motiv avem și acele caractere care ne sugerează final de linie (\\n). Dacă utilizăm print, atunci o să dispară acele caractere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a story about dogs\n",
      "our canine pets\n",
      "Dogs are furry animals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Din moment ce am zis că acest algoritm are de numărat fiecare cuvânt, ar fi ideal să avem aceste cuvite într-o listă separată"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'story',\n",
       " 'about',\n",
       " 'dogs',\n",
       " 'our',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'dogs',\n",
       " 'are',\n",
       " 'furry',\n",
       " 'animals']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.lower().split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În continuare o să creem un vocabular de cuvinte, vocabular care are o importanță extrem de mare în ceea ce privește Natural Language Processing. Acest dicționar o să conțină cuvintele unicare care apar în toate documentele. Pentru acest caz o să lucrăm cu două fișiere, One.txt, respectiv Two.txt și o să creem un set de valori unicate cu elementele din fiecare document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/18-Naive-Bayes-and-NLP/One.txt') as my_text:\n",
    "    words_one = my_text.read().lower().split()\n",
    "    uni_words_one = set(words_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/18-Naive-Bayes-and-NLP/Two.txt') as my_text:\n",
    "    words_two = my_text.read().lower().split()\n",
    "    uni_words_two = set(words_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'dogs',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'story',\n",
       " 'this'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_words_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'catching',\n",
       " 'fun',\n",
       " 'is',\n",
       " 'popular',\n",
       " 'sport',\n",
       " 'story',\n",
       " 'surfing',\n",
       " 'this',\n",
       " 'water',\n",
       " 'waves'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_words_two"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În acest moment avem câte un set de cuvinte pentru fiecare document în parte, ceea ce trebuie să facem este să creem un set care să conțină valorile unice din aceste două documente. Pentru asta o să utilizăm tot seturi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'catching',\n",
       " 'dogs',\n",
       " 'fun',\n",
       " 'furry',\n",
       " 'is',\n",
       " 'our',\n",
       " 'pets',\n",
       " 'popular',\n",
       " 'sport',\n",
       " 'story',\n",
       " 'surfing',\n",
       " 'this',\n",
       " 'water',\n",
       " 'waves'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_uni_words = set()\n",
    "all_uni_words.update(uni_words_one)\n",
    "all_uni_words.update(uni_words_two)\n",
    "all_uni_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Următorul pas este să atribuim fiecărui cuvânt câte un număr pentru a avea o referință la acele numere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for word in all_uni_words:\n",
    "    vocabulary[word]= i\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0,\n",
       " 'about': 1,\n",
       " 'our': 2,\n",
       " 'fun': 3,\n",
       " 'is': 4,\n",
       " 'story': 5,\n",
       " 'water': 6,\n",
       " 'waves': 7,\n",
       " 'catching': 8,\n",
       " 'dogs': 9,\n",
       " 'sport': 10,\n",
       " 'animals': 11,\n",
       " 'popular': 12,\n",
       " 'a': 13,\n",
       " 'furry': 14,\n",
       " 'are': 15,\n",
       " 'canine': 16,\n",
       " 'pets': 17,\n",
       " 'surfing': 18}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prin această modalitate acuma avem câte un identificator unic pentru fiecare cuvânt din acel set. În partea care urmează trebuie să creem trei liste separate, fiecare dintre ele având atâtea elemente câte elemente sunt în cadrul dicționarului respectiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_freq = [0] * len(vocabulary)\n",
    "two_freq = [0] * len(vocabulary)\n",
    "all_freq = [''] * len(vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listele de mai sus o să conțină valori care fac referire la fiecare apariție a cuvintelor din vocabular în cadrul unui document specific. De exemplu, lista 'one_freq' o să conțină numere care reprezintă de câte ori a apărut un anumit cuvânt în primul document. Lista all_freq o să conțină o lista cuvintelor din toate documentele aranjate după cum apar în cele două liste unde se verifică frecvența cuvintelor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/18-Naive-Bayes-and-NLP/One.txt') as file_one:\n",
    "    words_one = file_one.read().lower().split()\n",
    "\n",
    "for word in words_one:\n",
    "    word_index = vocabulary[word]\n",
    "    one_freq[word_index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0,\n",
       " 'about': 1,\n",
       " 'our': 2,\n",
       " 'fun': 3,\n",
       " 'is': 4,\n",
       " 'story': 5,\n",
       " 'water': 6,\n",
       " 'waves': 7,\n",
       " 'catching': 8,\n",
       " 'dogs': 9,\n",
       " 'sport': 10,\n",
       " 'animals': 11,\n",
       " 'popular': 12,\n",
       " 'a': 13,\n",
       " 'furry': 14,\n",
       " 'are': 15,\n",
       " 'canine': 16,\n",
       " 'pets': 17,\n",
       " 'surfing': 18}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "După cum se poate observa, în cadrul listei one_freq avem valori numerice de la 0 la 2. O valoare reprezintă de câte ori a apărut cuvântul din dicționar în acel document. De exemplu, cuvântul de pe index-ul 0 din dicționar îi corespunde valoarea de pe index-ul 0 din lista one_freq, prin urmare, cuvântul 'this' apare odată în primul document, cuvântul 'about' apare tot odată și tot așa. Același procedeu o să îl facem și pentru al doilea document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/18-Naive-Bayes-and-NLP/Two.txt') as file_two:\n",
    "    words_two = file_two.read().lower().split()\n",
    "\n",
    "for word in words_two:\n",
    "    word_index = vocabulary[word]\n",
    "    two_freq[word_index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 3, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_freq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Am făcut același lucru și cu cel de-al doilea document. Urmează să creem o listă cu toate cunvintele în ordinea în care apar acestea în dicționar. Această listă ne ajută la crearea unui DataFrame pentru vizualizare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary:\n",
    "    word_index = vocabulary[word]\n",
    "    all_freq[word_index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'about',\n",
       " 'our',\n",
       " 'fun',\n",
       " 'is',\n",
       " 'story',\n",
       " 'water',\n",
       " 'waves',\n",
       " 'catching',\n",
       " 'dogs',\n",
       " 'sport',\n",
       " 'animals',\n",
       " 'popular',\n",
       " 'a',\n",
       " 'furry',\n",
       " 'are',\n",
       " 'canine',\n",
       " 'pets',\n",
       " 'surfing']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_freq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "După ce avem și această listă putem să creem acel DataFrame pentru a vizualiza mai bine informațiile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>about</th>\n",
       "      <th>our</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>story</th>\n",
       "      <th>water</th>\n",
       "      <th>waves</th>\n",
       "      <th>catching</th>\n",
       "      <th>dogs</th>\n",
       "      <th>sport</th>\n",
       "      <th>animals</th>\n",
       "      <th>popular</th>\n",
       "      <th>a</th>\n",
       "      <th>furry</th>\n",
       "      <th>are</th>\n",
       "      <th>canine</th>\n",
       "      <th>pets</th>\n",
       "      <th>surfing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this  about  our  fun  is  story  water  waves  catching  dogs  sport  \\\n",
       "0     1      1    1    0   1      1      0      0         0     2      0   \n",
       "1     1      1    0    1   3      1      1      1         1     0      1   \n",
       "\n",
       "   animals  popular  a  furry  are  canine  pets  surfing  \n",
       "0        1        0  1      1    1       1     1        0  \n",
       "1        0        1  1      0    0       0     0        2  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data=[one_freq, two_freq], columns=all_freq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceea ce am creat mai sus poartă denumirea de 'bag of words model' care are scopul de a calcula frecvența tuturor cuvintelor din documente. Valorile pentru fiecare cuvânt în parte pot fi asemănătoare pentru toate documentele sau pot fi extrem de diferite. Prin acest procedeu am modificat text în valori numerice pe care putem să le oferim unui model de Machine Learning. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datele de mai sus nu sunt chiar așa de folositoare până ce nu introducem și conceptele de frecvența cuvintelor și cât de des apar aceste cuvinte în anumite documente. Dacă avem de-a face cu documente care se găsesc în același domeniu, de exemplu, toate documentele fac parte din categoria sport și încercăm să clasificăm documentele în sporturi (baschet, fotbal, tenis) există anumite cuvinte care apar în toate documentele, cum ar fi alergare sau minge, sau cuvinte care sunt specifice pentru un singur sport (coș la baschet, sau gol la fotbal). Pentru a face diferența dintre documente trebuie să luăm în calcul și acești factori."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În continuare o să creem acest algoritm cu Scikit-Learn, iar acesta ne permite să realizăm acest pas în doar câteva linii de cod."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
